#!/bin/bash
action=true . superdesk/master

root=$(pwd)
repo_backend=${repo_backend:-"$repo/server"}

_es_wait() {
    elastic=0
    while [ $elastic -eq 0 ]
    do
        curl -s "http://localhost:9200" 2>&1 > /dev/null \
            && elastic=1 \
            || echo "waiting for elastic..."
        sleep 1
    done
}

_es_init() {
    es_backups=/tmp/es-backups
    if [ ! -d "$es_backups" ]; then
        mkdir $es_backups
        chown elasticsearch:elasticsearch $es_backups
    fi

    echo 'log4j.rootLogger=OFF' > /etc/elasticsearch/logging.yml
    config='/etc/elasticsearch/elasticsearch.yml'
    cat << EOF > $config
$pattern
network.bind_host: 0.0.0.0
node.local: true
discovery.zen.ping.multicast: false
index.refresh_interval: 30s
index.store.type: memory
index.number_of_replicas: 0
path.repo: $es_backups

# Next setting break behave tests
# index.number_of_shards: 1
EOF

    systemctl restart elasticsearch
    _es_wait
    curl -XPUT 'http://localhost:9200/_snapshot/backups' \
        -d '{"type": "fs", "settings": {"location": "/tmp/es-backups"}}'
}

_mongo_init() {
    path=/tmp/mongodb
    [ -d $path ] || mkdir $path
    chown mongodb:mongodb $path
    cat << EOF > /etc/mongod.conf
storage:
  dbPath: $path
  journal:
    enabled: false
  engine: wiredTiger

net:
  port: 27017
  bindIp: 0.0.0.0
EOF
    systemctl restart mongod
}

_db_uninstall() {
    [ $(systemctl list-units | grep -c elasticsearch) = "0" ] && return 0
    systemctl stop elasticsearch mongod || true
    systemctl disable elasticsearch mongod || true
    apt-get -y remove \
        openjdk-8-jre-headless \
        elasticsearch \
        mongodb-org-server
    apt-get -y autoremove
}

_db_external() {
    [ -n "$lxc_data" ] || exit 1
    db_name=${db_name:-$(hostname)}
    MONGO_URI="mongodb://$lxc_data/$db_name"
    ELASTICSEARCH_URL="http://$lxc_data:9200"
    ELASTICSEARCH_INDEX="$db_name"

    AMAZON_S3_SUBFOLDER="$db_name"

    _envfile
    supervisorctl restart all

    if [ -n "$clean_data" ]; then
        curl -XDELETE  http://$lxc_data:9200/$db_name*

        apt install -y mongodb-clients
        mongo --host $lxc_data --eval "db.getMongo().getDBNames().forEach(function(v){\
            if(v.indexOf(\"$db_name\") === 0){db.getSiblingDB(v).dropDatabase()}\
        })"
    fi
}

_db_init() {
    if [ -n "$lxc_data" ]; then
        systemctl stop elasticsearch mongod
        _db_external
    else
        _es_init
        _mongo_init

        # initialize directories in "/tmp", etc.
        # TODO
        # hook=/opt/lxc-hooks/start.sh
        # hook_dir=$(dirname $hook)
        # mkdir -p $hook_dir
        # echo "mkdir /tmp/mongodb" > $hook
        # chmod +x $hook
    fi
}

_checks_init() {
    export SUPERDESK_TESTING=true
    _db_init

    path=/etc/supervisor/conf.d/${name}.conf
    . $root/superdesk-dev/supervisor.tpl > $path
    systemctl restart supervisor
}

_chrome_stable() {
    wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
    echo "deb http://dl.google.com/linux/chrome/deb/ stable main" \
        > /etc/apt/sources.list.d/google.list
    apt-get update
    apt-get install -y --no-install-recommends xvfb google-chrome-stable

    # don't need unexpected apt-get running
    rm -f /etc/cron.daily/google-chrome

    google-chrome --version
}

_chrome() {
    _chrome_stable
}

_backend_checks() {
    cd $repo_backend
    _activate

    # run flake8 with on jobs, because it could lock the build for ever
    [ -z "$flake8" ] || flake8 --jobs=1

    [ -z "$nose" ] || time nosetests -v --with-id

    feature=(./features/*.feature)
    [ -f $feature ] || behave=
    [ -z "$behave" ] || time behave --format progress2 --logging-clear-handlers --logcapture
}

_client_checks() {
    cd $repo/client
    [ -z "$npmtest" ] || time npm test
}

_log_mails() {
    cat <<EOF >> /etc/nginx/conf.d/locations
location /mail {
    return 302 $mails/;
}
EOF

    cat <<EOF >> /etc/supervisor/conf.d/mail.conf
[program:mail]
command=python3 -m api.smtp localhost 25 /var/log/$name/mail
autostart=true
autorestart=true
stdout_logfile=/var/log/$name/mail.log
redirect_stderr=true
directory=/opt/superdesk-fire
EOF

    supervisorctl update
    nginx -s reload
}

do_backend() {
    _venv
    cd $repo/server
    time pip install -r dev-requirements.txt
}

do_prepopulate() {
    [ -z "$lxc_data" ] && return 0
    curl -sI $lxc_data:9200/$db_name | grep 404 || return 0

    cd $repo/server

    _activate
    _sample_data

    # add default vocabularies
    ./manage.py app:initialize_data --entity-name vocabularies
    # add Forbes ingest source
    ./manage.py app:initialize_data --entity-name ingest_providers $sample_data
    # Use data from e2e tests
    ./manage.py app:prepopulate
}

do_finish() {
    _chrome

    # activate virtualenv by default
    # fix $PATH with local node modules
    cat <<EOF > /etc/profile.d/env.sh
. $env/bin/activate
PATH=./node_modules/.bin/:$PATH
EOF
}

do_checks() {
    if [ -z "$nose" ] && [ -z "$behave" ] && [ -z "$flake8" ] && [ -z "$npmtest" ]; then
        nose=1
        behave=1
        flake8=1
        npmtest=1
    fi
    _checks_init
    _backend_checks
    _client_checks
}

do_docs() { :; }

do_www() {
    _db_init
    sample_data=1 do_prepopulate

    _envfile
    _supervisor

    cd $(_repo_client)/dist
    sed -i \
        -e 's|iframely:{key:""}|iframely:{key:"'$IFRAMELY_KEY'"}|' \
        -e 's|raven:{dsn:""}|raven:{dsn:"'$SENTRY_DSN_PUBLIC'"}|' \
        app.bundle.*

    nginx_ssl=1 _nginx

    _log_mails
    do_docs || true
}

eval $action
